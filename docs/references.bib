@misc{baydin2018automaticdifferentiationmachinelearning,
      title={Automatic differentiation in machine learning: a survey}, 
      author={Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
      year={2018},
      eprint={1502.05767},
      archivePrefix={arXiv},
      primaryClass={cs.SC},
      url={https://arxiv.org/abs/1502.05767}, 
}

@misc{cs231n_backprop,
  author       = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
  title        = {Backpropagation},
  howpublished = {CS231n: Deep Learning for Computer Vision lecture notes},
  year         = {n.d.},
  url          = {https://cs231n.github.io/optimization-2/},
  note         = {Online; accessed 27-04-25}
}

@book{goodfellow2016deep,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  address   = {Cambridge, MA},
  url       = {https://www.deeplearningbook.org},
  note      = {Section 11.5 Debugging Strategies}
}

@misc{ufldl-debugging-gradient-checking,
  author       = {{Stanford University}},
  title        = {Debugging: Gradient Checking},
  howpublished = {\url{http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/}},
  note         = {Online; accessed 28-04-25}
}

@misc{Mehta2023Deriving,
  author       = {Mehta, Shivam},
  title        = {Deriving categorical cross entropy and softmax},
  howpublished = {Blog post},
  month        = {Jan},
  day          = {10},
  year         = {2023},
  url          = {https://shivammehta25.github.io/posts/deriving-categorical-cross-entropy-and-softmax/},
  note         = {Online; accessed 29-04-29}
}

@misc{wiki:Finite_difference,
   author = "Wikipedia",
   title = "{Finite difference} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2025",
   url = {http://en.wikipedia.org/w/index.php?title=Finite\%20difference&oldid=1285349475},
   note = "Online; accessed 29-04-2025"
 }

@article{chen2015mxnet,
  title   = {MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
  author  = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal = {CoRR},
  volume  = {abs/1512.01274},
  year    = {2015},
  url     = {http://arxiv.org/abs/1512.01274}
}

@online{dabbura2018coding,
  author    = {Dabbura, Imad},
  title     = {Coding Neural Network --- Gradient Checking},
  date      = {2018-04-08},
  journal   = {Medium},
  publisher = {Data Science},
  url       = {https://medium.com/data-science/coding-neural-network-gradient-checking-5222544ccc64},
  note      = {Online; accesed 29-04-25}
}

@ARTICLE{6296535,
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine}, 
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]}, 
  year={2012},
  volume={29},
  number={6},
  pages={141-142},
  keywords={Machine learning},
  doi={10.1109/MSP.2012.2211477}
}

@book{10.5555/2886323,
author = {Raschka, Sebastian},
title = {Python Machine Learning},
year = {2015},
chapter  = {12},
isbn = {1783555130},
publisher = {Packt Publishing},
abstract = {Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.}
}

@article{rumelhart1986learning,
  added-at = {2023-03-05T10:30:55.000+0100},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  biburl = {https://www.bibsonomy.org/bibtex/2197dd0ba9ef6a582f0219953dc315f39/jascal_panetzky},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {197dd0ba9ef6a582f0219953dc315f39},
  journal = {nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2023-03-05T10:34:04.000+0100},
  title = {Learning representations by back-propagating errors},
  volume = 323,
  year = 1986
}

@Inbook{LeCun2012,
author="LeCun, Yann A.
and Bottou, L{\'e}on
and Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--48",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_3",
url="https://doi.org/10.1007/978-3-642-35289-8_3"
}

@misc{harrison2021briefintroductionautomaticdifferentiation,
      title={A Brief Introduction to Automatic Differentiation for Machine Learning}, 
      author={Davan Harrison},
      year={2021},
      eprint={2110.06209},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.06209}, 
}